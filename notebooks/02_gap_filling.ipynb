{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03cc212b-6d53-4ddb-bdeb-556c8a0bbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno  # installed using: `pip install missingno`\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09d5332-3aaf-49be-9093-029ae4c65728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOWA\n",
      "TOWB\n",
      "TOWD\n",
      "TOWF\n",
      "TOWS\n",
      "TOWY\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "tower_dfs_15m = []\n",
    "tower_dfs_60m = []\n",
    "towers_of_interest = ['TOWA', 'TOWB', 'TOWD', 'TOWF', 'TOWS', 'TOWY']\n",
    "\n",
    "# populate df lists\n",
    "for tower in towers_of_interest:\n",
    "\n",
    "    print(tower)\n",
    "    \n",
    "    #15-minute data\n",
    "    df_15m = pd.read_csv(f'../data/met_towers_2017-2022_final-qc/{tower}_2017-2022_final-qc.csv',\n",
    "                         index_col=0, header=0, na_values=['-999.0', '#DIV/0!', -999])\n",
    "    df_15m.index = pd.to_datetime(df_15m.index, format='%Y%m%d%H%M%S', utc=True)\n",
    "    \n",
    "    # 60-minute data\n",
    "    df_60m = pd.read_csv(f'../data/met_towers_2017-2022_hourly-qc/{tower}_2017-2022_hourly-qc.csv',\n",
    "                         index_col=0, header=0, na_values=['-999.0', '#DIV/0!', -999])\n",
    "    df_60m.index = pd.to_datetime(df_60m.index, format='%Y%m%d%H%M%S', utc=True)\n",
    "    \n",
    "    tower_dfs_15m.append(df_15m)\n",
    "    tower_dfs_60m.append(df_60m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7489d8-1a38-4c6f-b58f-730e012a59cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AbsHum_015m</th>\n",
       "      <th>BarPresMb_015m</th>\n",
       "      <th>PkWSpdMph_015m</th>\n",
       "      <th>PkWSpdMph_030m</th>\n",
       "      <th>PrecipIn_015m</th>\n",
       "      <th>RelHum_015m</th>\n",
       "      <th>Sigma_015m</th>\n",
       "      <th>Sigma_030m</th>\n",
       "      <th>SigPhi_015m</th>\n",
       "      <th>SigPhi_030m</th>\n",
       "      <th>TempC_015m</th>\n",
       "      <th>TempC_030m</th>\n",
       "      <th>VSSpdMph_015m</th>\n",
       "      <th>VSSpdMph_030m</th>\n",
       "      <th>WDir_015m</th>\n",
       "      <th>WDir_030m</th>\n",
       "      <th>WSpdMph_015m</th>\n",
       "      <th>WSpdMph_030m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestampUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 05:00:00+00:00</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>985.20000</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>97.40000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>15.300000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>261.00000</td>\n",
       "      <td>265.00000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 05:15:00+00:00</th>\n",
       "      <td>6.021160</td>\n",
       "      <td>985.20325</td>\n",
       "      <td>6.473885</td>\n",
       "      <td>8.820410</td>\n",
       "      <td>0.038205</td>\n",
       "      <td>97.33393</td>\n",
       "      <td>27.355656</td>\n",
       "      <td>14.914644</td>\n",
       "      <td>15.871311</td>\n",
       "      <td>10.189119</td>\n",
       "      <td>3.710002</td>\n",
       "      <td>3.764628</td>\n",
       "      <td>-0.099206</td>\n",
       "      <td>-0.098456</td>\n",
       "      <td>259.69922</td>\n",
       "      <td>263.71222</td>\n",
       "      <td>2.325163</td>\n",
       "      <td>4.172241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 05:30:00+00:00</th>\n",
       "      <td>6.045611</td>\n",
       "      <td>985.20013</td>\n",
       "      <td>6.141583</td>\n",
       "      <td>7.728682</td>\n",
       "      <td>0.035806</td>\n",
       "      <td>97.29949</td>\n",
       "      <td>28.260035</td>\n",
       "      <td>14.289950</td>\n",
       "      <td>15.693998</td>\n",
       "      <td>9.653637</td>\n",
       "      <td>3.791788</td>\n",
       "      <td>3.816717</td>\n",
       "      <td>-0.099093</td>\n",
       "      <td>-0.098235</td>\n",
       "      <td>260.69196</td>\n",
       "      <td>264.20682</td>\n",
       "      <td>2.187686</td>\n",
       "      <td>3.984347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           AbsHum_015m  BarPresMb_015m  PkWSpdMph_015m  \\\n",
       "timestampUTC                                                             \n",
       "2017-01-01 05:00:00+00:00     6.000000       985.20000        6.700000   \n",
       "2017-01-01 05:15:00+00:00     6.021160       985.20325        6.473885   \n",
       "2017-01-01 05:30:00+00:00     6.045611       985.20013        6.141583   \n",
       "\n",
       "                           PkWSpdMph_030m  PrecipIn_015m  RelHum_015m  \\\n",
       "timestampUTC                                                            \n",
       "2017-01-01 05:00:00+00:00       10.200000       0.040000     97.40000   \n",
       "2017-01-01 05:15:00+00:00        8.820410       0.038205     97.33393   \n",
       "2017-01-01 05:30:00+00:00        7.728682       0.035806     97.29949   \n",
       "\n",
       "                           Sigma_015m  Sigma_030m  SigPhi_015m  SigPhi_030m  \\\n",
       "timestampUTC                                                                  \n",
       "2017-01-01 05:00:00+00:00   25.400000   15.300000    15.700000    10.600000   \n",
       "2017-01-01 05:15:00+00:00   27.355656   14.914644    15.871311    10.189119   \n",
       "2017-01-01 05:30:00+00:00   28.260035   14.289950    15.693998     9.653637   \n",
       "\n",
       "                           TempC_015m  TempC_030m  VSSpdMph_015m  \\\n",
       "timestampUTC                                                       \n",
       "2017-01-01 05:00:00+00:00    3.600000    3.700000      -0.100000   \n",
       "2017-01-01 05:15:00+00:00    3.710002    3.764628      -0.099206   \n",
       "2017-01-01 05:30:00+00:00    3.791788    3.816717      -0.099093   \n",
       "\n",
       "                           VSSpdMph_030m  WDir_015m  WDir_030m  WSpdMph_015m  \\\n",
       "timestampUTC                                                                   \n",
       "2017-01-01 05:00:00+00:00      -0.100000  261.00000  265.00000      2.500000   \n",
       "2017-01-01 05:15:00+00:00      -0.098456  259.69922  263.71222      2.325163   \n",
       "2017-01-01 05:30:00+00:00      -0.098235  260.69196  264.20682      2.187686   \n",
       "\n",
       "                           WSpdMph_030m  \n",
       "timestampUTC                             \n",
       "2017-01-01 05:00:00+00:00      4.400000  \n",
       "2017-01-01 05:15:00+00:00      4.172241  \n",
       "2017-01-01 05:30:00+00:00      3.984347  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tower_dfs_60m[0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293b543c-86bf-4d3b-8cff-30067d869e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TempC_015m</th>\n",
       "      <th>TempC_030m</th>\n",
       "      <th>RelHum_015m</th>\n",
       "      <th>AbsHum_015m</th>\n",
       "      <th>WSpdMph_015m</th>\n",
       "      <th>WSpdMph_030m</th>\n",
       "      <th>PkWSpdMph_015m</th>\n",
       "      <th>PkWSpdMph_030m</th>\n",
       "      <th>VSSpdMph_015m</th>\n",
       "      <th>VSSpdMph_030m</th>\n",
       "      <th>BarPresMb_015m</th>\n",
       "      <th>Sigma_015m</th>\n",
       "      <th>Sigma_030m</th>\n",
       "      <th>SigPhi_015m</th>\n",
       "      <th>SigPhi_030m</th>\n",
       "      <th>WDir_015m</th>\n",
       "      <th>WDir_030m</th>\n",
       "      <th>PrecipIn_015m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestampUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 05:00:00+00:00</th>\n",
       "      <td>3.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>97.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>985.3</td>\n",
       "      <td>26.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>18.9</td>\n",
       "      <td>12.6</td>\n",
       "      <td>268.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.015748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 05:15:00+00:00</th>\n",
       "      <td>3.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>97.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>985.3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>13.9</td>\n",
       "      <td>14.3</td>\n",
       "      <td>9.8</td>\n",
       "      <td>252.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.011811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 05:30:00+00:00</th>\n",
       "      <td>3.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>97.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>985.2</td>\n",
       "      <td>25.3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.011811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           TempC_015m  TempC_030m  RelHum_015m  AbsHum_015m  \\\n",
       "timestampUTC                                                                  \n",
       "2017-01-01 05:00:00+00:00         3.6         3.6         97.4          6.0   \n",
       "2017-01-01 05:15:00+00:00         3.6         3.6         97.3          6.0   \n",
       "2017-01-01 05:30:00+00:00         3.6         3.7         97.4          6.0   \n",
       "\n",
       "                           WSpdMph_015m  WSpdMph_030m  PkWSpdMph_015m  \\\n",
       "timestampUTC                                                            \n",
       "2017-01-01 05:00:00+00:00           2.2           4.4             5.9   \n",
       "2017-01-01 05:15:00+00:00           2.4           4.7             6.7   \n",
       "2017-01-01 05:30:00+00:00           2.8           4.3             6.0   \n",
       "\n",
       "                           PkWSpdMph_030m  VSSpdMph_015m  VSSpdMph_030m  \\\n",
       "timestampUTC                                                              \n",
       "2017-01-01 05:00:00+00:00            10.2            0.0      -0.100000   \n",
       "2017-01-01 05:15:00+00:00             7.7            0.0      -0.100000   \n",
       "2017-01-01 05:30:00+00:00             6.8           -0.2       0.000064   \n",
       "\n",
       "                           BarPresMb_015m  Sigma_015m  Sigma_030m  \\\n",
       "timestampUTC                                                        \n",
       "2017-01-01 05:00:00+00:00           985.3        26.5        15.5   \n",
       "2017-01-01 05:15:00+00:00           985.3        19.3        13.9   \n",
       "2017-01-01 05:30:00+00:00           985.2        25.3        15.0   \n",
       "\n",
       "                           SigPhi_015m  SigPhi_030m  WDir_015m  WDir_030m  \\\n",
       "timestampUTC                                                                \n",
       "2017-01-01 05:00:00+00:00         18.9         12.6      268.0      274.0   \n",
       "2017-01-01 05:15:00+00:00         14.3          9.8      252.0      264.0   \n",
       "2017-01-01 05:30:00+00:00         13.6         11.0      255.0      260.0   \n",
       "\n",
       "                           PrecipIn_015m  \n",
       "timestampUTC                              \n",
       "2017-01-01 05:00:00+00:00       0.015748  \n",
       "2017-01-01 05:15:00+00:00       0.011811  \n",
       "2017-01-01 05:30:00+00:00       0.011811  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tower_dfs_15m[0].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a52832-bc7f-4277-a379-19eb9afb70c1",
   "metadata": {},
   "source": [
    "## 1. Fill gaps with hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908f2a27-bfc0-4adc-8d87-cde6c8669c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOWA\n",
      "TOWB\n",
      "TOWD\n",
      "TOWF\n",
      "TOWS\n",
      "TOWY\n"
     ]
    }
   ],
   "source": [
    "# loop through four 15-min dataframes, 60-min dataframes, and tower names\n",
    "tdfs_15m_gapfilled = []\n",
    "for i, (df_15m, df_60m, tname) in enumerate(zip(tower_dfs_15m, tower_dfs_60m, towers_of_interest)):\n",
    "\n",
    "    print(tname)\n",
    "\n",
    "    # empty list to be filled with gap-filled columns\n",
    "    gap_filled_cols = []\n",
    "    \n",
    "    ##################################################################\n",
    "    # find consecutive gaps in each column\n",
    "    ##################################################################\n",
    "\n",
    "    # get missing time periods\n",
    "    for col_name in df_15m.columns.to_list():\n",
    "\n",
    "        length = []\n",
    "        dstart = []\n",
    "        dend = []\n",
    "        idxs = []\n",
    "        \n",
    "        # print(f'{tname}: {col_name}')\n",
    "        missing_data = df_15m[col_name].isnull()\n",
    "        consecutive_missing = []\n",
    "        for column, g in groupby(enumerate(missing_data), lambda x: x[1]):\n",
    "            if column:\n",
    "                consecutive_missing.append(list(map(lambda x: x[0], list(g))))\n",
    "    \n",
    "        # save missing time periods\n",
    "        for lst in consecutive_missing:\n",
    "    \n",
    "            # gaps in time\n",
    "            steps = len(lst)\n",
    "            mins = steps*15\n",
    "            hours = mins/60\n",
    "            start = lst[0]\n",
    "            end = lst[-1]\n",
    "    \n",
    "            # append gap lengths, start of gap, end of gap, idxs\n",
    "            length.append(hours)\n",
    "            dstart.append(df_15m.iloc[start,:].name)\n",
    "            dend.append(df_15m.iloc[end,:].name)\n",
    "            idxs.append(lst)\n",
    "    \n",
    "        ##################################################################\n",
    "        # gap-fill each column\n",
    "        ##################################################################\n",
    "        \n",
    "        # create dataframe of missing data\n",
    "        missing_data = pd.DataFrame(length, columns=['gap_length_hrs'])\n",
    "        missing_data['tower'] = tname\n",
    "        missing_data['colname'] = col_name\n",
    "        missing_data['gap_start_date'] = dstart\n",
    "        missing_data['gap_end_date'] = dend\n",
    "        missing_data['indexes'] = idxs\n",
    "    \n",
    "        # group gaps into gap types: small, medium, large\n",
    "        mid_gaps = missing_data[(missing_data['gap_length_hrs'] >= 3) & (missing_data['gap_length_hrs'] <= 6)]\n",
    "        big_gaps = missing_data[missing_data['gap_length_hrs'] > 6]\n",
    "        sml_gaps = missing_data[missing_data['gap_length_hrs'] < 3]\n",
    "\n",
    "        #-----------------------------------------------------------------#\n",
    "        # (1) fill gaps with hourly data\n",
    "        main_gaps = pd.concat([sml_gaps, mid_gaps, big_gaps])\n",
    "        \n",
    "        # flatten 2D list of NaN indices\n",
    "        idx_lst = main_gaps['indexes'].to_list()\n",
    "        idx_lst = [x for lst in idx_lst for x in lst]\n",
    "\n",
    "        # subselect dataframe to current column\n",
    "        df_15m_col = df_15m[[col_name]]\n",
    "        try:\n",
    "            df_60m_col = df_60m[[col_name]]\n",
    "        except Exception:\n",
    "            # print(f'**ERROR** No 60-min column {col_name}')\n",
    "            pass\n",
    "\n",
    "        # fill gaps with hourly data\n",
    "        try:\n",
    "            df_60m_sel = df_60m_col.iloc[idx_lst]\n",
    "            df_15m_col = df_15m_col.fillna(df_60m_sel) # filling gaps with hourly\n",
    "        except Exception:\n",
    "            # print(f'**UNKNOWN INDEX ERROR** for {col_name}')\n",
    "            pass\n",
    "\n",
    "        gap_filled_cols.append(df_15m_col)\n",
    "\n",
    "    # concatenate filled columns column-wise (axis=1)\n",
    "    new_df = pd.concat(gap_filled_cols, axis=1)\n",
    "    tdfs_15m_gapfilled.append(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5715f-bf91-472d-9ff0-e3b09847287d",
   "metadata": {},
   "source": [
    "## 2. Fill small gaps with Linear and PCHIP interpolation\n",
    "PCHIP performs visually better than plain linear interpolation; linear interpolation creates new outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f72f81-e26d-4860-8f03-4ef71938d129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOWA\n",
      "TOWB\n",
      "TOWD\n",
      "TOWF\n",
      "TOWS\n",
      "TOWY\n"
     ]
    }
   ],
   "source": [
    "# Add linear interpolation comparison\n",
    "tdfs_15m_gapfilled_pchip = []\n",
    "tdfs_15m_gapfilled_linear = []\n",
    "\n",
    "def add_buffer(df_15m, col_name, buffer_hours=3):\n",
    "    df_15m.index = pd.to_datetime(df_15m.index)\n",
    "    mean_buffer = df_15m.groupby([df_15m.index.month, df_15m.index.day, df_15m.index.hour])[col_name].median()\n",
    "    \n",
    "    start_date = df_15m.index[0] - pd.Timedelta(hours=buffer_hours)\n",
    "    start_buffer = [mean_buffer[(start_date.month, start_date.day, hour)] for hour in range(start_date.hour, start_date.hour + buffer_hours)]\n",
    "    \n",
    "    end_date = df_15m.index[-1] + pd.Timedelta(hours=buffer_hours)\n",
    "    end_buffer = [mean_buffer[(end_date.month, end_date.day, hour)] for hour in range(end_date.hour - buffer_hours + 1, end_date.hour + 1)]\n",
    "    \n",
    "    buffer_start = pd.Series(start_buffer, index=pd.date_range(start=start_date, periods=buffer_hours, freq='h'))\n",
    "    buffer_end = pd.Series(end_buffer, index=pd.date_range(start=df_15m.index[-1] + pd.Timedelta(hours=1), periods=buffer_hours, freq='h'))\n",
    "    \n",
    "    df_with_buffer = pd.concat([buffer_start, df_15m[col_name], buffer_end])\n",
    "    \n",
    "    return df_with_buffer\n",
    "\n",
    "def rolling_clip(series, window=48, threshold=1.5):\n",
    "    rolling_median = series.rolling(window=window, min_periods=1, center=True).median()\n",
    "    rolling_std = series.rolling(window=window, min_periods=1, center=True).std()\n",
    "    lower_bound = rolling_median - threshold * rolling_std\n",
    "    upper_bound = rolling_median + threshold * rolling_std\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "def global_clip(series, lower_percentile=1, upper_percentile=99):\n",
    "    lower_bound = series.quantile(lower_percentile / 100.0)\n",
    "    upper_bound = series.quantile(upper_percentile / 100.0)\n",
    "    return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "for i, (df_15m, tname) in enumerate(zip(tdfs_15m_gapfilled, towers_of_interest)):\n",
    "\n",
    "    print(tname)\n",
    "    gap_filled_cols_pchip = []\n",
    "    gap_filled_cols_linear = []\n",
    "\n",
    "    for col_name in df_15m.columns.to_list():\n",
    "        \n",
    "        df_15m_col = df_15m[[col_name]].copy()\n",
    "\n",
    "        try:\n",
    "            # Smooth the data before interpolation\n",
    "            df_with_buffer = add_buffer(df_15m_col, col_name, buffer_hours=3)\n",
    "            smoothed_col = savgol_filter(df_with_buffer.values, window_length=11, polyorder=2, mode='nearest')\n",
    "            smoothed_df = pd.DataFrame(smoothed_col, index=df_with_buffer.index, columns=[col_name])\n",
    "\n",
    "            # PCHIP interpolation\n",
    "            smoothed_df_pchip = smoothed_df.copy()\n",
    "            smoothed_df_pchip[col_name] = smoothed_df_pchip[col_name].interpolate(method='pchip', limit=12, limit_direction='both')\n",
    "            smoothed_df_pchip[col_name] = global_clip(smoothed_df_pchip[col_name], lower_percentile=1, upper_percentile=99)\n",
    "            smoothed_df_pchip[col_name] = rolling_clip(smoothed_df_pchip[col_name])\n",
    "            df_15m_col[col_name] = df_15m_col[col_name].fillna(smoothed_df_pchip[col_name])\n",
    "            gap_filled_cols_pchip.append(df_15m_col)\n",
    "\n",
    "            # Linear interpolation\n",
    "            df_15m_col_linear = df_15m[[col_name]].copy()\n",
    "            smoothed_df_linear = smoothed_df.copy()\n",
    "            smoothed_df_linear[col_name] = smoothed_df_linear[col_name].interpolate(method='linear', limit=12, limit_direction='both')\n",
    "            smoothed_df_linear[col_name] = global_clip(smoothed_df_linear[col_name], lower_percentile=1, upper_percentile=99)\n",
    "            smoothed_df_linear[col_name] = rolling_clip(smoothed_df_linear[col_name])\n",
    "            df_15m_col_linear[col_name] = df_15m_col_linear[col_name].fillna(smoothed_df_linear[col_name])\n",
    "            gap_filled_cols_linear.append(df_15m_col_linear)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Interpolation failed for {col_name}: {e}\")\n",
    "            pass\n",
    "\n",
    "    new_df_pchip = pd.concat(gap_filled_cols_pchip, axis=1)\n",
    "    tdfs_15m_gapfilled_pchip.append(new_df_pchip)\n",
    "\n",
    "    new_df_linear = pd.concat(gap_filled_cols_linear, axis=1)\n",
    "    tdfs_15m_gapfilled_linear.append(new_df_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f89087-319e-4062-91c3-72aa37966a04",
   "metadata": {},
   "source": [
    "## Cap values based on limits table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d288892-2be7-4a41-ad98-b5de46882382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap values (again) using limits table\n",
    "limits = pd.read_csv('../data/supplementary/met_inst_ranges.csv')\n",
    "limits.set_index('Sensor', drop=True, inplace=True)\n",
    "oor = limits[limits['Error Type'] == 'Out of Range Values'] # oor = out of range\n",
    "oor = oor[['Min', 'Max']]\n",
    "\n",
    "# Prerform the capping:\n",
    "for df in tdfs_15m_gapfilled_linear:\n",
    "    # Iterate over each sensor in the `oor` table\n",
    "    for sensor, limits in oor.iterrows():\n",
    "        min_val, max_val = limits['Min'], limits['Max']\n",
    "        \n",
    "        # Find columns in the DataFrame that start with the sensor string\n",
    "        matching_columns = [col for col in df.columns if col.startswith(sensor)]\n",
    "        \n",
    "        # Apply capping to the matching columns\n",
    "        for col in matching_columns:\n",
    "            if np.isfinite(min_val):  # Apply min limit if it's not -inf\n",
    "                df[col] = df[col].clip(lower=min_val)\n",
    "            if np.isfinite(max_val):  # Apply max limit if it's not inf\n",
    "                df[col] = df[col].clip(upper=max_val)\n",
    "\n",
    "for df in tdfs_15m_gapfilled_pchip:\n",
    "    # Iterate over each sensor in the `oor` table\n",
    "    for sensor, limits in oor.iterrows():\n",
    "        min_val, max_val = limits['Min'], limits['Max']\n",
    "        \n",
    "        # Find columns in the DataFrame that start with the sensor string\n",
    "        matching_columns = [col for col in df.columns if col.startswith(sensor)]\n",
    "        \n",
    "        # Apply capping to the matching columns\n",
    "        for col in matching_columns:\n",
    "            if np.isfinite(min_val):  # Apply min limit if it's not -inf\n",
    "                df[col] = df[col].clip(lower=min_val)\n",
    "            if np.isfinite(max_val):  # Apply max limit if it's not inf\n",
    "                df[col] = df[col].clip(upper=max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339d12f-2eb4-423a-a2fa-5cf35f5a4a8a",
   "metadata": {},
   "source": [
    "## 3. Visualize original vs gap-filled data using MSNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c01a6f-cf4b-401e-bf85-002f205f838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column renaming guide\n",
    "label_conversions = {\n",
    "    'TempC': 'Temperature (°C)',\n",
    "    'RelHum': 'Relative Humidity (%)',\n",
    "    'AbsHum': 'Absolute Humidity (g/m\\N{SUPERSCRIPT THREE})',\n",
    "    'WSpdMph': 'Lateral Wind Speed (mph)',\n",
    "    'PkWSpdMph': 'Peak Lateral Wind Speed (mph)',\n",
    "    'VSSpdMph': 'Vertical Wind Speed (mph)',\n",
    "    'BarPresMb': 'Pressure (mb)',\n",
    "    'Sigma': r'$\\sigma_\\theta$ (degree)',\n",
    "    'SigPhi': r'$\\sigma_\\phi$ (degree)',\n",
    "    'WDir': 'Wind Direction (degree)',\n",
    "    'PrecipIn': 'Precipitation (in)',\n",
    "    'SolarRadWm2': 'Solar Radiation (W/m\\N{SUPERSCRIPT TWO})'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1372c3e-c993-4df7-829c-55c965ae75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to rename columns based on the guide and remove leading zeros in height\n",
    "def rename_columns(df, conversions):\n",
    "    # For each column, check if it matches a pattern and rename accordingly\n",
    "    renamed_columns = {}\n",
    "    for col in df.columns:\n",
    "        new_name = col\n",
    "        for key, value in conversions.items():\n",
    "            if key in col:  # Check for a match\n",
    "                # Extract measurement height if present (e.g., _015m)\n",
    "                if \"_\" in col:\n",
    "                    parts = col.split(\"_\")\n",
    "                    if len(parts) > 1 and parts[1].endswith(\"m\"):\n",
    "                        height = parts[1].lstrip(\"0\")\n",
    "                        height = re.sub(r\"(\\d+)m\", r\"\\1-m\", height)\n",
    "                        new_name = f\"{value} at {height}\"\n",
    "                    else:\n",
    "                        new_name = value\n",
    "                else:\n",
    "                    new_name = value\n",
    "        renamed_columns[col] = new_name\n",
    "    \n",
    "    # Return a new DataFrame with renamed columns\n",
    "    return df.rename(columns=renamed_columns, inplace=False)\n",
    "\n",
    "# Loop through pairs of dataframes and rename columns\n",
    "renamed_dfs_15m = []  # List to store renamed DataFrames for tdf\n",
    "renamed_dfs_15m_gf_linear = []  # List to store renamed DataFrames for tdf_gf\n",
    "renamed_dfs_15m_gf_pchip = []\n",
    "\n",
    "for tdf, tdf_gf_linear, tdf_gf_pchip in zip(tower_dfs_15m, tdfs_15m_gapfilled_linear, tdfs_15m_gapfilled_pchip):\n",
    "    renamed_tdf = rename_columns(tdf, label_conversions)      # Get renamed DataFrame for tdf\n",
    "    renamed_tdf_gf_linear = rename_columns(tdf_gf_linear, label_conversions)  # Get renamed DataFrame for tdf_gf\n",
    "    renamed_tdf_gf_pchip = rename_columns(tdf_gf_pchip, label_conversions)\n",
    "    \n",
    "    # Add the renamed DataFrames to the lists\n",
    "    renamed_dfs_15m.append(renamed_tdf)\n",
    "    renamed_dfs_15m_gf_linear.append(renamed_tdf_gf_linear)\n",
    "    renamed_dfs_15m_gf_pchip.append(renamed_tdf_gf_pchip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ee1501-909c-4df5-9c32-f6f35b447dfa",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca71d365-d681-4601-96c1-dfac44ad3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parentheses_from_labels(labels):\n",
    "    \"\"\"Remove parentheses and their content from a list of labels.\"\"\"\n",
    "    return [re.sub(r'\\s*\\(.*?\\)', '', label) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c16fb82-bd77-45d7-bf35-aecd7f253ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_data_with_sparkline(data, ax_matrix, ax_sparkline, title):\n",
    "    \"\"\"Plot the missingno matrix and a custom sparkline for missing data.\"\"\"\n",
    "    # Temporarily rename columns for plotting (remove parentheses)\n",
    "    original_columns = data.columns\n",
    "    cleaned_columns = remove_parentheses_from_labels(original_columns)\n",
    "    data.columns = cleaned_columns\n",
    "\n",
    "    # Plot missingno matrix\n",
    "    msno.matrix(data, ax=ax_matrix, sparkline=False, fontsize=16, color=(0.5, 0.5, 0.5))\n",
    "    # Adjust the title to be fully left-justified\n",
    "    ax_matrix.set_title(title, fontsize=24, loc='left', x=-0.09)\n",
    "    \n",
    "    # Remove y-axis tick labels from the matrix plot\n",
    "    y_positions = [-0.5, 35040, 70080, 105120, 140160, 175296, 210335.5]\n",
    "    years = [2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "    for tick, year in zip(y_positions, years):\n",
    "        if year not in [2017, 2023]:\n",
    "            ax_matrix.axhline(y=tick, c='black', ls='--', lw=1)\n",
    "    ax_matrix.set_yticks(y_positions)\n",
    "    ax_matrix.set_yticklabels(years)\n",
    "    \n",
    "    # Calculate missing values per timestamp and plot as a sparkline\n",
    "    missing_counts = data.isna().sum(axis=1)\n",
    "    ax_sparkline.plot(missing_counts, missing_counts.index, color=(0.5, 0.7, 0.5))\n",
    "    ax_sparkline.set_title('Missing Data Sparkline', fontsize=14)\n",
    "    ax_sparkline.set_xlabel('Missing Count', fontsize=14)\n",
    "    ax_sparkline.set_xlim(0, len(data.columns))\n",
    "    \n",
    "    # Reverse the y-axis so the oldest year is at the top\n",
    "    ax_sparkline.invert_yaxis()\n",
    "    ax_sparkline.grid(axis='y', color='black', ls='--', lw=1)\n",
    "    ax_sparkline.set_axisbelow(False)\n",
    "    ax_sparkline.set_yticklabels([])\n",
    "\n",
    "    # Restore original column names\n",
    "    data.columns = original_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1794a144-8113-49ba-be67-20203dd7c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image to ../graphics/gapfilling/missing_data_plots/TOWA_2017–2022.png\n",
      "Saving image to ../graphics/gapfilling/missing_data_plots/TOWB_2017–2022.png\n",
      "Saving image to ../graphics/gapfilling/missing_data_plots/TOWD_2017–2022.png\n",
      "Saving image to ../graphics/gapfilling/missing_data_plots/TOWF_2017–2022.png\n",
      "Saving image to ../graphics/gapfilling/missing_data_plots/TOWS_2017–2022.png\n",
      "Saving image to ../graphics/gapfilling/missing_data_plots/TOWY_2017–2022.png\n"
     ]
    }
   ],
   "source": [
    "# Combined plot of non-gapfilled and gap-filled data\n",
    "for tdf_15m, gapfilled_tdf, tname in zip(renamed_dfs_15m, renamed_dfs_15m_gf_linear, towers_of_interest):\n",
    "\n",
    "    # Sort columns to ensure consistency between original and gap-filled data\n",
    "    tdf_15m = tdf_15m[sorted(tdf_15m.columns)]\n",
    "    gapfilled_tdf = gapfilled_tdf[sorted(gapfilled_tdf.columns)]\n",
    "\n",
    "    # Adjust figure width to better accommodate labels\n",
    "    fig = plt.figure(figsize=(16, 20))  # Increased width (16) for better spacing\n",
    "    gs = GridSpec(2, 2, width_ratios=[1, 0.3], wspace=0.1, hspace=0.7)\n",
    "\n",
    "    # First pair: Original data\n",
    "    ax_matrix_1 = fig.add_subplot(gs[0, 0])\n",
    "    ax_matrix_1.set_ylabel('Year', fontsize=16, labelpad=10)\n",
    "    ax_sparkline_1 = fig.add_subplot(gs[0, 1])\n",
    "    plot_missing_data_with_sparkline(tdf_15m, ax_matrix_1, ax_sparkline_1, f'(A) {tname} Original Data')\n",
    "\n",
    "    # Second pair: Gap-filled data\n",
    "    ax_matrix_2 = fig.add_subplot(gs[1, 0])\n",
    "    ax_matrix_2.set_ylabel('Year', fontsize=16, labelpad=10)\n",
    "    ax_sparkline_2 = fig.add_subplot(gs[1, 1])\n",
    "    plot_missing_data_with_sparkline(gapfilled_tdf, ax_matrix_2, ax_sparkline_2, f'(B) {tname} Gap-filled Data')\n",
    "\n",
    "    # Improve label spacing\n",
    "    for ax in [ax_matrix_1, ax_matrix_2]:\n",
    "        for label in ax.get_xticklabels():\n",
    "            label.set_rotation(45)  # Rotate labels for better visibility\n",
    "        ax.tick_params(axis='y', labelsize=14, pad=1)\n",
    "\n",
    "    # Save figure\n",
    "    save_path = f'../graphics/gapfilling/missing_data_plots/{tname}_2017–2022.png'\n",
    "    directory = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    print(f'Saving image to {save_path}')\n",
    "\n",
    "    # Save at high resolution\n",
    "    fig.savefig(save_path, bbox_inches='tight', dpi=600)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe8598-c9bc-4997-aeaa-61fec099432e",
   "metadata": {},
   "source": [
    "## 4. Visualize raw timeseries to visually assess gap-filling performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa08cda4-cd51-4a1a-b55e-f41fb2e1ef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE values written to ../data/pchip_vs_linear_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"../data/pchip_vs_linear_comparison.csv\"\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow(['tower', 'variable', 'height', 'pchip_rmse', 'linear_rmse'])\n",
    "\n",
    "    for tdf_15m, tdf_15m_gapfilled_pchip, tdf_15m_gapfilled_linear, tower_name in zip(renamed_dfs_15m, renamed_dfs_15m_gf_pchip, renamed_dfs_15m_gf_linear, towers_of_interest):\n",
    "        # Loop through each column in the dataframe\n",
    "        for column in tdf_15m.columns:\n",
    "            variable, height = column.split(' ')[0], column.split(' ')[-1].split('-')[0]\n",
    "\n",
    "            # Generate a new random mask for each plot\n",
    "            mask = np.random.choice([True, False], size=tdf_15m[column].shape, p=[0.8, 0.2])\n",
    "\n",
    "            # Mask the test data by assigning NaNs (instead of dropping values)\n",
    "            train_data = tdf_15m[column].copy()\n",
    "            test_data = train_data[~mask]\n",
    "            train_data[~mask] = np.nan  # Replace test data points with NaN in the training set\n",
    "\n",
    "            # Ensure the index is monotonic for PCHIP\n",
    "            train_data_sorted = train_data.sort_index()\n",
    "\n",
    "            # Apply interpolation on the training set with masked NaNs\n",
    "            pchip_filled = train_data_sorted.interpolate(method='pchip')\n",
    "            linear_filled = train_data_sorted.interpolate(method='linear')\n",
    "\n",
    "            # Ensure no NaNs in test_data and corresponding interpolated values\n",
    "            test_data_nonan = test_data.dropna()\n",
    "            pchip_filled_nonan = pchip_filled.loc[test_data_nonan.index].dropna()\n",
    "            linear_filled_nonan = linear_filled.loc[test_data_nonan.index].dropna()\n",
    "\n",
    "            # Ensure matching indices\n",
    "            common_index = test_data_nonan.index.intersection(pchip_filled_nonan.index)\n",
    "\n",
    "            # Align both datasets to this common index\n",
    "            test_data_aligned = test_data_nonan.loc[common_index]\n",
    "            pchip_filled_aligned = pchip_filled_nonan.loc[common_index]\n",
    "            linear_filled_aligned = linear_filled_nonan.loc[common_index]\n",
    "\n",
    "            # Calculate RMSE for PCHIP and Linear interpolation\n",
    "            rmse_pchip = np.sqrt(mean_squared_error(test_data_aligned, pchip_filled_aligned))\n",
    "            rmse_linear = np.sqrt(mean_squared_error(test_data_aligned, linear_filled_aligned))\n",
    "            rmse_pchip_rounded = round(rmse_pchip, 4)\n",
    "            rmse_linear_rounded = round(rmse_linear, 4)\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow([tower_name, variable, height, rmse_pchip_rounded, rmse_linear_rounded])\n",
    "\n",
    "print(f\"RMSE values written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a06d108-6677-47ed-b0a8-5881d30ee3d1",
   "metadata": {},
   "source": [
    "## 4. Export gap-filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a04375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export file\n",
    "export_file = tdfs_15m_gapfilled_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51e21ad5-6429-42e8-8f63-41c066ad8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export gap-filled data\n",
    "for tdf, tower in zip(export_file, towers_of_interest):\n",
    "    tdf.index = tdf.index.strftime('%Y%m%d%H%M%S')\n",
    "    tdf = tdf.fillna(-999)\n",
    "    tdf = tdf.astype('float32')\n",
    "    tdf.to_csv(f'../data/met_towers_2017-2022_gapfilled-qc/{tower}_2017-2022_gapfilled-qc.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f88a21d-0e6a-4889-8d3b-a9922566f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export boolean table\n",
    "# Example table: was a value gap-filled? true = yes; false = no\n",
    "((tower_dfs_15m[0].fillna(-999)).ne((export_file[0].fillna(-999)))).head(5)\n",
    "for df_orig, df_filled, tower in zip(tower_dfs_15m, export_file, towers_of_interest):\n",
    "    df_orig.index = df_orig.index.strftime('%Y%m%d%H%M%S')\n",
    "    bool_df = (df_orig.fillna(-999)).ne(df_filled.fillna(-999))\n",
    "    bool_df.columns.name = None\n",
    "    bool_df = bool_df.astype('float32')\n",
    "    bool_df.to_csv(f'../data/met_towers_2017-2022_gapfilled-bool/{tower}_2017-2022_gapfilled-bool.csv',\n",
    "                   index_label='timestampUTC', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ebf02-e28d-46f7-8491-139ff53cecd1",
   "metadata": {},
   "source": [
    "## 5. Compare original data with gap-filled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574eabbf-fbb9-4011-b71b-e37863f5488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # overwrite original-qc file to fix the timestamp column\n",
    "# # only needed to do this once\n",
    "# for tower in towers_of_interest:\n",
    "#     df = (\n",
    "#         pd.read_csv(f'../data/met_towers_2017-2022_original-qc/{tower}_2017-2022_original-qc.csv')\n",
    "#         .assign(timestampUTC=lambda x: pd.to_datetime(x['Timestamp'], format='%Y-%m-%d %H:%M:%S'))\n",
    "#         .set_index('timestampUTC')\n",
    "#     )\n",
    "#     df.index = df.index.tz_localize('US/Eastern', ambiguous='NaT', nonexistent='NaT').tz_convert('UTC').strftime('%Y%m%d%H%M%S')\n",
    "#     df.drop(columns=['Timestamp'], inplace=True)\n",
    "#     df.to_csv(f'../data/met_towers_2017-2022_original-qc/{tower}_2017-2022_original-qc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3114a36-c1e9-4388-b832-31908e88657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tower A original data\n",
    "example_tower = 'TOWD'\n",
    "\n",
    "og_df = pd.read_csv(f'../data/met_towers_2017-2022_original-qc/{example_tower}_2017-2022_original-qc.csv', na_values=[-999])\n",
    "og_df2 = rename_columns(og_df, label_conversions)\n",
    "\n",
    "# Tower A gap-filled data\n",
    "gf_df = pd.read_csv(f'../data/met_towers_2017-2022_gapfilled-qc/{example_tower}_2017-2022_gapfilled-qc.csv', na_values=[-999])\n",
    "gf_df2 = rename_columns(gf_df, label_conversions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79218ff5-feaa-44d1-ad8a-3d63690306cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_temperature.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_2-m_2017-2022_temperature.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_35-m_2017-2022_temperature.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_60-m_2017-2022_temperature.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_relative_humidity.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_2-m_2017-2022_relative_humidity.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_absolute_humidity.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_2-m_2017-2022_absolute_humidity.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_lateral_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_35-m_2017-2022_lateral_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_60-m_2017-2022_lateral_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_peak_lateral_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_35-m_2017-2022_peak_lateral_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_60-m_2017-2022_peak_lateral_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_vertical_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_60-m_2017-2022_vertical_wind_speed.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_2-m_2017-2022_solar_radiation.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_pressure.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_2-m_2017-2022_pressure.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_sigma_theta.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_35-m_2017-2022_sigma_theta.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_60-m_2017-2022_sigma_theta.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_sigma_phi.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_60-m_2017-2022_sigma_phi.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_15-m_2017-2022_wind_direction.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_35-m_2017-2022_wind_direction.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_60-m_2017-2022_wind_direction.png\n",
      "Saving image to ../graphics/gapfilling/pchip_vs_linear/TOWD_2-m_2017-2022_precipitation.png\n"
     ]
    }
   ],
   "source": [
    "# Ensure directory exists\n",
    "if not os.path.isdir('../graphics/gapfilling/pchip_vs_linear'):\n",
    "    os.mkdir('../graphics/gapfilling/pchip_vs_linear')\n",
    "\n",
    "# Ensure a common time column for plotting\n",
    "time_column = 'timestampUTC'  # Adjust this to match your actual time column name\n",
    "if time_column in gf_df2.columns:\n",
    "    gf_df2[time_column] = pd.to_datetime(gf_df2[time_column], format='%Y%m%d%H%M%S')\n",
    "if time_column in og_df2.columns:\n",
    "    og_df2[time_column] = pd.to_datetime(og_df2[time_column], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Extract the tower letter from the global variable `example_tower`\n",
    "tower_letter = example_tower[-1]  # Extract the last character from `example_tower`\n",
    "\n",
    "# Iterate through columns in gf_df (ignoring the time column)\n",
    "for col in gf_df2.columns:\n",
    "    if col == time_column:\n",
    "        continue  # Skip time column\n",
    "    \n",
    "    # Extract variable name and parentheses for y-axis label\n",
    "    y_label_match = re.match(r'^[^()]+(?:\\s*\\([^)]*\\))?', col)  # Match variable name with parentheses\n",
    "    y_label = y_label_match.group(0).strip() if y_label_match else col  # Includes parentheses\n",
    "\n",
    "    # Extract variable name without parentheses for title\n",
    "    y_label_no_parentheses = re.sub(r'\\s*\\(.*?\\)', '', y_label).strip()  # Removes parentheses and content inside\n",
    "\n",
    "    # Extract sensor height (e.g., \"15-m\") if it exists\n",
    "    height_match = re.search(r'at\\s([\\d\\w-]+)', col)  # Match \"at 15-m\" and extract \"15-m\"\n",
    "    height = height_match.group(1) if height_match else \"unknown\"\n",
    "\n",
    "    # Start plotting\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(6, 8), sharex=True)\n",
    "    \n",
    "    # Plot from og_df if the column exists, otherwise show placeholder text\n",
    "    if col in og_df2.columns:\n",
    "        axes[0].plot(og_df2[time_column], og_df2[col], color=(0.4, 0.6, 0.8), lw=0.5)\n",
    "        axes[0].set_title(f\"(A) Original Tower {tower_letter} data at {height} sensor height\")\n",
    "    else:\n",
    "        axes[0].text(0.5, 0.5, 'Column data was not present', \n",
    "                     horizontalalignment='center', verticalalignment='center', \n",
    "                     transform=axes[0].transAxes, fontsize=12, color='red')\n",
    "        axes[0].set_title(f\"(A) Original Tower {tower_letter} data at {height} sensor height\")\n",
    "    \n",
    "    # Plot from gf_df\n",
    "    axes[1].plot(gf_df2[time_column], gf_df2[col], color=(0.4, 0.6, 0.8), lw=0.5)\n",
    "    axes[1].set_title(f\"(B) Gap-filled Tower {tower_letter} data at {height} sensor height\")\n",
    "\n",
    "    # Add labels and finalize formatting\n",
    "    axes[1].set_xlabel(\"Year\")\n",
    "    axes[0].set_ylabel(y_label)  # Keep parentheses in the y-axis label\n",
    "    axes[1].set_ylabel(y_label)  # Keep parentheses in the y-axis label\n",
    "\n",
    "    # Set the y-axis limits for both plots to be the same\n",
    "    min_y = min(og_df2[col].min(), gf_df2[col].min())\n",
    "    max_y = max(og_df2[col].max(), gf_df2[col].max())\n",
    "    axes[0].set_ylim(min_y, max_y)\n",
    "    axes[1].set_ylim(min_y, max_y)\n",
    "\n",
    "    # Save figure\n",
    "    varname = y_label_no_parentheses.lower().replace(' ', '_')\n",
    "    varname = varname.replace(\"\\\\\", \"\").replace(\"$\", \"\")\n",
    "    save_path = f'../graphics/gapfilling/pchip_vs_linear/TOW{tower_letter}_{height}_2017-2022_{varname}.png'\n",
    "    directory = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    print(f'Saving image to {save_path}')\n",
    "\n",
    "    # Save at high resolution\n",
    "    fig.savefig(save_path, bbox_inches='tight', dpi=600)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a96d69-25fe-46d5-b073-f1416ee79488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of names to check for in column names\n",
    "# names = [\n",
    "#     \"DT\", \"MixHeight\", \"MixRatio\", \"SatMixRatio\", \"SatVaporPres\", \"SonicTemp\", \"StabSigPhi\", \"StabSRDT\", \n",
    "#     \"UWind\", \"VaporPres\", \"VWDir\", \"VWind\", \"WElev\", \"SWSpdMph\", \"SWSpdMs\", \"RWSpdMph\", \"RWSpdMs\", \n",
    "#     \"RSWSpdMph\", \"RSWSpdMs\", \"SPkWSpdMph\", \"SPkWSpdMs\", \"SWDir\", \"SVWDir\", \"SSigma\", \"DT002m015m\", \n",
    "#     \"DT002m035m\", \"DT002m060m\", \"DT015m035m\", \"DT015m060m\", \"SoilTemp\", \"Ri015m035m\", \"Ri015m060m\", \n",
    "#     \"Stab\", \"DeltaTemp\", \"LHV\"\n",
    "# ]\n",
    "\n",
    "# # Initialize a dictionary to store the results\n",
    "# results = {}\n",
    "\n",
    "# # Loop through each DataFrame in tdfs_15m_gapfilled_linear\n",
    "# for idx, df in enumerate(tdfs_15m_gapfilled_linear):\n",
    "#     # Dictionary to store random samples for the current DataFrame\n",
    "#     df_samples = {}\n",
    "    \n",
    "#     # Loop through each column in the DataFrame\n",
    "#     for column in df.columns:\n",
    "#         # Check if the column name contains any string from the names list\n",
    "#         if any(name in column for name in names):\n",
    "#             print(column)\n",
    "#             # Extract non-NaN values from the column\n",
    "#             non_nan_values = df[column].dropna().tolist()\n",
    "            \n",
    "#             # If there are at least 10 non-NaN values, take a random sample\n",
    "#             if len(non_nan_values) >= 10:\n",
    "#                 df_samples[column] = random.sample(non_nan_values, 10)\n",
    "    \n",
    "#     # Store the samples for the current DataFrame\n",
    "#     if df_samples:  # Only include DataFrames with matching columns\n",
    "#         results[f\"DataFrame_{idx}\"] = df_samples\n",
    "\n",
    "# # Example: Print or process the results\n",
    "# for df_key, sampled_columns in results.items():\n",
    "#     print(f\"{df_key}:\")\n",
    "#     for column, samples in sampled_columns.items():\n",
    "#         print(f\"  {column}: {samples}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsrd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
